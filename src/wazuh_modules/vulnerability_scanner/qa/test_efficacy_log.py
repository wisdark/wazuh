import json
import platform
import subprocess
import socket
import os
import time
import threading
from pathlib import Path
import glob

import pytest
from jsonschema import validate
from jsonschema.exceptions import ValidationError

import logging

LOGGER = logging.getLogger(__name__)
import re
import time

isDeltas = False
isRsync = False

def find_regex_in_file(regex, file, times=1, max_timeout=50):
    pattern = re.compile(regex)
    start_time = time.time()

    while time.time() - start_time < max_timeout:
        count = 0
        with open(file, 'r') as f:
            content = f.read()
            count = len(pattern.findall(content))
            LOGGER.debug(f"Found {count} matches")
            if count == times:
                return True

        time.sleep(0.1)
    return False

def sendflatbuffer_to_unixsocket(data):
    # Create a unix socket
    sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    # Connect to the socket
    try:
        # Check if the socket exists
        if isDeltas == True:
            if not Path("queue/router/deltas-syscollector").exists():
                LOGGER.info(f"Socket does not exists")
                return None
            sock.connect("queue/router/deltas-syscollector")
        elif isRsync == True:
            if not Path("queue/router/rsync-syscollector").exists():
                LOGGER.info(f"Socket does not exists")
                return None
            sock.connect("queue/router/rsync-syscollector")
        else:
            LOGGER.info(f"Socket does not exists")
            return None
        size = len(data)+5;
        data_to_send = size.to_bytes(4, byteorder="little")
        header_size = 1
        data_to_send += header_size.to_bytes(4, byteorder="little")
        data_to_send += b"P"
        data_to_send += data

        # Send the data
        sock.send(data_to_send)

    except Exception as e:
        LOGGER.info(f"Socket error {e}")
        return None


def json2binary(test, output):
    command = ["external/flatbuffers/build/flatc", "--binary", "-o", output, "shared_modules/utils/flatbuffers/schemas/syscollectorDeltas/syscollector_deltas.fbs", test]
    # Execute the flatbuffer compiler
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)

    global isDeltas
    global isRsync

    # if the result is not 0, test with another fbs schema.
    if result.returncode != 0:
        command = ["external/flatbuffers/build/flatc", "--binary", "-o", output, "shared_modules/utils/flatbuffers/schemas/syscollectorRsync/syscollector_synchronization.fbs", test]
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        if result.returncode == 0:
            isRsync = True
            isDeltas = False
            LOGGER.debug(f"Is rsync")
    else:
        isDeltas = True
        isRsync = False
        LOGGER.debug(f"Is deltas")

    # if the result is not 0, stop the test
    assert result.returncode == 0, f"Error: {result.stdout}"

def tail_log(file, expected_lines, found_lines, timeout):
    start_time = time.time()
    with open(file, "r") as f:
        while not all(found_lines.values()) and (time.time() - start_time <= timeout):
            line = f.readline()
            if not line:
                continue
            # Check if the line contains the expected output
            for expected in expected_lines:
                if expected in line and not found_lines[expected]:
                    LOGGER.info(f"Found log line: {line}")
                    found_lines[expected] = True

@pytest.fixture
def run_on_end(request):
    yield
    # Read the location of the log
    if 'GITHUB_WORKSPACE' not in os.environ:
        LOGGER.info("GITHUB_WORKSPACE is not defined")
        return
    path = os.environ['GITHUB_WORKSPACE']
    # Search for the log.out file in path variable
    for file_path in glob.glob(f'{path}/**/log.out', recursive=True):
        # Copy the file found to another directory
        LOGGER.info(f"Copying {file_path} to {path}/qa_logs/log.out.{request.node.name}")
        os.system(f"cp {file_path} {path}/qa_logs/log.out.{request.node.name}")

@pytest.fixture
def run_process_and_monitor_log(request, run_on_end):
    # Delete previous inventory directory if exists
    if Path("queue/vd/inventory").exists():
        for file in Path("queue/vd/inventory").glob("*"):
            file.unlink()
        Path("queue/vd/inventory").rmdir()

    # Set the path to the binary
    cmd = Path("build/wazuh_modules/vulnerability_scanner/testtool/scanner/", "vd_scanner_testtool")
    cmdAlt = Path("wazuh_modules/vulnerability_scanner/build/testtool/scanner/", "vd_scanner_testtool")

    # Ensure the binary exists
    if not cmd.exists():
        cmd = cmdAlt
    assert cmd.exists(), "The binary does not exists"

    args = ["-c", "wazuh_modules/vulnerability_scanner/testtool/scanner/config.json",
            "-t", "wazuh_modules/vulnerability_scanner/indexer/template/index-template.json",
            "-l", "log.out",
            "-s", "120",
            "-h", "wazuh_modules/vulnerability_scanner/testtool/scanner/fakeAgentData/agentHotfixesData.json",
            "-b", "wazuh_modules/vulnerability_scanner/testtool/scanner/fakeGlobalData/globalData.json",
            "-u"]

    command = [cmd] + args
    test_folder = request.param

    LOGGER.debug(f"Running test {test_folder}")

    # Remove previous log file if exists
    if Path("log.out").exists():
        Path("log.out").unlink()

    # Iterate over json files in the test directory, convert to flatbuffer and send through unix socket
    with subprocess.Popen(command) as process:
        # Check if the log file exists, if the line is not found, try again in 1 second
        start_time = time.time()
        log_file = "log.out"
        while not Path(log_file).exists() and (time.time() - start_time <= 10):
            time.sleep(1)

        assert Path(log_file).exists(), "The log file does not exists"

        # Check if the process is initialized finding in the log file a specific line.
        if test_folder.name == '000':
            LOGGER.debug(f"Waiting for the decompression to start.")
            found = find_regex_in_file(r"Starting database file decompression.", log_file)
            assert found, "The decompression of the DB did not start."
            LOGGER.info(f"Decompression started")
        else:
            LOGGER.debug(f"Wating for the process to be initialized")
            found = find_regex_in_file(r"Vulnerability scanner module started", log_file)
            assert found, "The process is not initialized, timeout waiting vulnerability scanner module to start."
            LOGGER.info(f"Process initialized")

        expected_json_files = sorted(Path(test_folder).glob("expected_*.out"))
        expected_lines = []
        # Read expected output if it exists, this is an json with and array of lines.
        for expected_json_file in expected_json_files:
            # Parse json and add the string elements of te array to the expected lines
            json_data = None
            try:
                json_data = json.load(open(expected_json_file))
            except:
                process.terminate()
                pytest.exit("Couldn't parse JSON file")

            for line in json_data:
                expected_lines.append(line)

        LOGGER.debug(f"Expected lines: {expected_lines}")
        quantity_expected_lines = len(expected_lines)
        LOGGER.debug(f"Quantity expected lines: {quantity_expected_lines}")

        found_lines = {line: False for line in expected_lines}

        json_files = sorted(Path(test_folder).glob("input_*.json"))

        for json_file in json_files:
            LOGGER.debug(f"Running test {json_file}")
            with open(json_file) as f:
                # Set the output file
                file = str(json_file)

                # Parse json file and print the data
                json_data = json.load(open(file))

                # Set the output folder
                output_folder = str(test_folder)

                # Set the output file
                output = str(json_file).replace(".json", ".bin")

                # Convert the json data to flatbuffer
                json2binary(file, output_folder)

                # Read the flatbuffer data
                with open(output, "rb") as f:
                    flatbuffer_data = f.read()

                LOGGER.debug("Sending flatbuffer data")

                # After start to read lines, send the flatbuffer data
                sendflatbuffer_to_unixsocket(flatbuffer_data)

        # Wait until the scan is finished, the scan is finished with this line: "Vulnerability scan for package '*' on Agent '*' has completed." using regex.
        if not test_folder.name == '000':
            regex = r"Event type: (.*) processed"
            found = find_regex_in_file(regex, log_file, len(expected_json_files))
            regex = r"Discarded event: DB query not synced"
            found = found or find_regex_in_file(regex, log_file, len(expected_json_files))
            assert found, "The scan is not finished, some events were not processed"
            LOGGER.info(f"Scan finished, all events were processed")

        timeout = 1
        for expected_line in expected_lines:
            while not found_lines[expected_line]:
                LOGGER.debug(f"Waiting for log line: {expected_line}")
                if timeout < 60:
                    tail_log(log_file, expected_lines, found_lines, timeout)
                    timeout = 2*timeout
                    if request.param.parent.name == 'test_false_positive_data' and timeout > 3:
                        break
                else:
                    LOGGER.error(f"Timeout waiting for log line: {expected_line}")
                    timeout = 1
                    break

        process.terminate()

    LOGGER.debug(f"Waiting for the process to finish")
    return found_lines

test_false_negative_folders = sorted(Path("wazuh_modules/vulnerability_scanner/qa/test_false_negative_data").glob(os.getenv('WAZUH_VD_TEST_FN_GLOB', '*')))
test_false_positive_folders = sorted(Path("wazuh_modules/vulnerability_scanner/qa/test_false_positive_data").glob(os.getenv('WAZUH_VD_TEST_FP_GLOB', '*')))

@pytest.mark.parametrize("run_process_and_monitor_log", test_false_negative_folders, indirect=True)
def test_false_negatives(run_process_and_monitor_log):
    # Change working directory to the root of the project parent directory.
    # This is required to run the binary.
    os.chdir(Path(__file__).parent.parent.parent.parent)

    found_lines = run_process_and_monitor_log
    for line, found in found_lines.items():
        if not found:
            LOGGER.error(f"Log entry not found: {line}")
    assert all(found_lines.values()), "The test failed because some expected lines were not found"

@pytest.mark.skip(reason="Test failing skipped until fix implemented")
@pytest.mark.parametrize("run_process_and_monitor_log", test_false_positive_folders, indirect=True)
def test_false_positives(run_process_and_monitor_log):
    # Change working directory to the root of the project parent directory.
    # This is required to run the binary.
    os.chdir(Path(__file__).parent.parent.parent.parent)

    found_lines = run_process_and_monitor_log
    for line, found in found_lines.items():
        assert not found, f"The test failed because some unexpected line ({line}) was found."
